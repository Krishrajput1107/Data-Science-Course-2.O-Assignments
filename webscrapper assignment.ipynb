{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5560eb7-df71-49b4-babd-9a3a87b47720",
   "metadata": {},
   "source": [
    "1.Web scraping is an automated process of extracting data from websites. It is used for data collection and analysis, price monitoring, content aggregation, weather data retrieval, financial data gathering, lead generation, job market analysis, and social media monitoring. However, it must be done ethically and responsibly, respecting website guidelines and avoiding any harm to the targeted sites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4035e-937b-40b9-9173-640a4e2e55e7",
   "metadata": {},
   "source": [
    "2.Web scraping methods include parsing HTML with libraries like BeautifulSoup, using web scraping libraries like Scrapy and Cheerio, accessing APIs for structured data, using headless browsers like Puppeteer and Selenium, employing regular expressions (Regex), DOM parsing, and using web scraping services. Ethical practices should always be followed to respect website terms and avoid overloading servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528c913-13d0-4e74-b978-1ee010779a39",
   "metadata": {},
   "source": [
    "3.Beautiful Soup is a Python library used for parsing HTML and XML documents. It simplifies web scraping by providing an easy-to-use interface to navigate the page's structure and extract data from specific elements, attributes, or text content. It handles messy HTML/XML, integrates well with other Python libraries, and has strong community support, making it a popular choice for web scraping tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf208a1-b277-43d4-a1fd-cc86a3151126",
   "metadata": {},
   "source": [
    "4.Flask is used in web scraping projects for creating APIs, data visualization, handling asynchronous tasks, database integration, scalability, and customization. Its simplicity and flexibility make it an excellent choice for building web scraping applications and services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c2f03-3f48-4330-bdf2-f0bf6340bdad",
   "metadata": {},
   "source": [
    "5.AWS services used in a web scraping project:\n",
    "\n",
    "1. EC2: Scalable virtual servers for running web scraping scripts.\n",
    "2. Lambda: Serverless execution of web scraping tasks based on events or triggers.\n",
    "3. S3: Storage for the scraped data.\n",
    "4. API Gateway: Exposing scraped data as RESTful APIs.\n",
    "5. DynamoDB: NoSQL database for structured scraped data.\n",
    "6. CloudWatch: Monitoring and logging AWS resources.\n",
    "7. SQS: Asynchronous processing and decoupling of components.\n",
    "8. CloudFront: Content delivery network for caching and distributing scraped data.\n",
    "9. ECS: Managing containers for web scraping applications.\n",
    "10. Glue: Automated data discovery, cataloging, and transformation.\n",
    "\n",
    "These services enhance scalability, efficiency, and reliability in the web scraping project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7a443-4761-42fd-8405-c0df0ac90e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
